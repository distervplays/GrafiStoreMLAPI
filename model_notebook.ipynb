{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.UpdateDatabase import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from urllib.parse import urljoin\n",
    "from dotenv import load_dotenv\n",
    "from datetime import *\n",
    "from bson import ObjectId\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import random\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4314,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.FlattenData import *\n",
    "from modules.PremoAPI import *\n",
    "from utils.SafeDataConverters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4315,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PremoAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4316,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getenv(\"mongodb_user\")\n",
    "password = os.getenv(\"mongodb_password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.database import Database\n",
    "db = Database(username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4318,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_data: list[dict] = api.play_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4319,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_out = flatten_output(db.to_json())\n",
    "flat_in = flatten_input(db.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workspace Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(flat_out).sort_values(by=['order_id', 'sort_order'], ascending=True)\n",
    "out_df['sort_order'] = out_df.groupby(['order_id', 'product_id'])['sort_order'].transform(lambda x: x.rank(method='dense').astype(int) - 1)\n",
    "# out_df = out_df.map(safe_to_datetime)\n",
    "out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df[['delivery_date', 'order_created_at', 'start_at', 'end_at']] = out_df[['delivery_date', 'order_created_at', 'start_at', 'end_at']].apply(pd.to_datetime, format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "out_df['duration_since_order_created'] = out_df['start_at'] - out_df['order_created_at']\n",
    "out_df['duration_since_order_created'] = out_df['duration_since_order_created'].dt.total_seconds() / 3600 # convert to hours\n",
    "out_df['time_until_delivery'] = out_df['delivery_date'] - out_df['start_at']\n",
    "out_df['time_until_delivery'] = out_df['time_until_delivery'].dt.total_seconds() / 3600 # convert to hours\n",
    "out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4323,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns = {\n",
    "    'init': ['task_title'],\n",
    "    'addit': ['material', 'color', 'sort_order']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4324,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_columns = ['workspace', 'duration_since_order_created', 'time_until_delivery']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = out_df.copy()[['task_title', 'delivery_date', 'order_created_at', 'material', 'color', 'sort_order', 'task_duration',  'start_at', 'end_at','workspace', 'duration_since_order_created', 'time_until_delivery']]\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_inputs(df):\n",
    "    # Compute time differences in hours\n",
    "    df['delivery_offset'] = (df['delivery_date'] - df['order_created_at']).dt.total_seconds() / 3600.0\n",
    "    \n",
    "    # Extract day of the week and time of day\n",
    "    df['order_day_of_week'] = df['order_created_at'].dt.weekday  # 0 = Monday\n",
    "    df['order_time_of_day'] = df['order_created_at'].dt.hour + df['order_created_at'].dt.minute / 60.0\n",
    "    \n",
    "    # Normalize or scale features as needed\n",
    "    # Example normalization\n",
    "    df['delivery_offset'] = df['delivery_offset'] / df['delivery_offset'].max()\n",
    "    df['task_duration'] = df['task_duration'] / df['task_duration'].max()\n",
    "    df['order_day_of_week'] = df['order_day_of_week'] / 6.0  # Since weekdays range from 0 to 6\n",
    "    df['order_time_of_day'] = (df['order_time_of_day'] - 9) / 8.0  # Scale between 0 and 1\n",
    "    \n",
    "    # Select features\n",
    "    X = df[['delivery_offset', 'task_duration', 'order_day_of_week', 'order_time_of_day']].values\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_outputs(df):\n",
    "    # Ensure datetime conversion\n",
    "    df['start_at'] = pd.to_datetime(df['start_at'], errors='coerce')\n",
    "    df['order_created_at'] = pd.to_datetime(df['order_created_at'], errors='coerce')\n",
    "    \n",
    "    # Handle any NaT values\n",
    "    df = df.dropna(subset=['start_at', 'order_created_at'])\n",
    "    \n",
    "    # Compute date offset (ensure it's at least 1)\n",
    "    df['date_offset'] = (df['start_at'].dt.date - df['order_created_at'].dt.date).dt.days\n",
    "    df['date_offset'] = df['date_offset'].apply(lambda x: max(1, x))\n",
    "    \n",
    "    # Compute start time in hours\n",
    "    df['start_time'] = df['start_at'].dt.hour + df['start_at'].dt.minute / 60.0\n",
    "    \n",
    "    y = df[['date_offset', 'start_time']].values\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4329,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnTransformer:\n",
    "    def __init__(self, categorical_features: list[str], numerical_features: list[str]):\n",
    "        self.categorical_features = categorical_features\n",
    "        self.numerical_features = numerical_features\n",
    "        self.one_hot_encoders = {}\n",
    "        self.scalers = {}\n",
    "        self._feature_names_out = []\n",
    "        self._feature_names_in = []\n",
    "        \n",
    "    def fit(self, X: pd.DataFrame) -> None:\n",
    "        for feature in self.categorical_features:\n",
    "            if feature not in X.columns:\n",
    "                continue\n",
    "            ohe = OneHotEncoder(sparse_output=False)\n",
    "            ohe.fit(X[[feature]])\n",
    "            self.one_hot_encoders[feature] = ohe\n",
    "            \n",
    "            self._feature_names_out.extend(ohe.get_feature_names_out([feature]))\n",
    "            self._feature_names_in.append(feature)\n",
    "            \n",
    "        for feature in self.numerical_features:\n",
    "            if feature not in X.columns:\n",
    "                continue\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(X[[feature]])\n",
    "            self.scalers[feature] = scaler\n",
    "            \n",
    "            self._feature_names_in.append(feature)\n",
    "            self._feature_names_out.append(feature)\n",
    "            \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X_out = []\n",
    "        for feature in self.categorical_features:\n",
    "            if feature not in X.columns:\n",
    "                continue\n",
    "            ohe = self.one_hot_encoders[feature]\n",
    "            transformed = ohe.transform(X[[feature]])\n",
    "            X_out.append(pd.DataFrame(transformed, \n",
    "                                        columns=ohe.get_feature_names_out([feature]), \n",
    "                                        index=X.index))\n",
    "            \n",
    "        for feature in self.numerical_features:\n",
    "            if feature not in X.columns:\n",
    "                continue\n",
    "            scaler = self.scalers[feature]\n",
    "            transformed = scaler.transform(X[[feature]])\n",
    "            X_out.append(pd.DataFrame(transformed, columns=[feature], index=X.index))\n",
    "        \n",
    "        for feature in X.columns:\n",
    "            if feature not in self._feature_names_in:\n",
    "                X_out.append(X[[feature]])\n",
    "        \n",
    "        return pd.concat(X_out, axis=1)\n",
    "    \n",
    "    def fit_transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X_out = []\n",
    "        for feature in self.categorical_features:\n",
    "            ohe_feature_names = [col for col in X.columns if col.startswith(feature + '_')]\n",
    "            if not ohe_feature_names:\n",
    "                continue\n",
    "            ohe = self.one_hot_encoders[feature]\n",
    "            X_out.append(pd.DataFrame(ohe.inverse_transform(X[ohe_feature_names]), columns=[feature], index=X.index))\n",
    "            \n",
    "        for feature in self.numerical_features:\n",
    "            if feature not in X.columns:\n",
    "                continue\n",
    "            scaler = self.scalers[feature]\n",
    "            X_out.append(pd.DataFrame(scaler.inverse_transform(X[[feature]]), columns=[feature], index=X.index))\n",
    "            \n",
    "        for feature in X.columns:\n",
    "            if feature not in self._feature_names_out:\n",
    "                X_out.append(X[[feature]])\n",
    "            \n",
    "        if not X_out:\n",
    "            raise ValueError(\"No objects to concatenate\")\n",
    "            \n",
    "        return pd.concat(X_out, axis=1)\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.one_hot_encoders = {}\n",
    "        self.scalers = {}\n",
    "        self._feature_names_out = []\n",
    "        self._feature_names_in = []\n",
    "    \n",
    "    def get_feature_names_out(self) -> list[str]:\n",
    "        return self._feature_names_out\n",
    "    \n",
    "    def get_feature_names_in(self) -> list[str]:\n",
    "        return self._feature_names_in\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4330,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['task_title', 'material', 'color', 'workspace']\n",
    "# numerical_features = ['duration_since_order_created', 'time_until_delivery']\n",
    "numerical_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4331,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(categorical_features, numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['duration_since_order_created', 'time_until_delivery', 'start_at', 'end_at'])\n",
    "# y = df[['duration_since_order_created', 'time_until_delivery']]\n",
    "y = df.copy()[['start_at', 'end_at']] # .apply(lambda x: x.astype('int64') // 10**9).astype('float32')\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4333,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ct.fit_transform(X)\n",
    "# y = ct.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4334,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df['start_at'] = pd.to_datetime(out_df['start_at'], errors='coerce')\n",
    "out_df['end_at'] = pd.to_datetime(out_df['end_at'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X = preprocess_inputs(out_df)\n",
    "y = preprocess_outputs(out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract lists for order_created_at and task_duration\n",
    "order_created_at_list = df['order_created_at'].tolist()\n",
    "task_duration_list = df['task_duration'].tolist()\n",
    "\n",
    "# Initialize and train the model\n",
    "model_manager = Model()\n",
    "trained_model, history = model_manager.train(X, y)\n",
    "\n",
    "# Make predictions\n",
    "X_test = preprocess_inputs(df_test)  # Assuming df_test is your test DataFrame\n",
    "order_created_at_test = df_test['order_created_at'].tolist()\n",
    "task_duration_test = df_test['task_duration'].tolist()\n",
    "\n",
    "predictions = model_manager.predict(X_test, order_created_at_test, task_duration_test)\n",
    "\n",
    "# Display predictions\n",
    "for start_at, end_at in predictions:\n",
    "    print(f\"Start At: {start_at}, End At: {end_at}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3964,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3965,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3966,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.num_epochs = 100\n",
    "# agent.learning_rate = 0.001\n",
    "# agent.batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = agent.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3968,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot total loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3970,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_flat = flatten_input(play_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_df = pd.DataFrame(play_flat).sort_values(by=['order_id', 'sort_order'], ascending=True).convert_dtypes()\n",
    "play_df['sort_order'] = play_df.groupby(['order_id', 'material'])['sort_order'].transform(lambda x: x.rank(method='dense').astype(int) - 1)\n",
    "play_df = play_df.groupby(['order_id', 'material']).apply(lambda x: x.sort_values(by='sort_order')).reset_index(drop=True)\n",
    "play_df = play_df.map(safe_to_datetime)\n",
    "play_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_play = play_df[['task_title', 'material', 'color', 'sort_order', 'workspace']]\n",
    "X_play.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_play = ct.transform(X_play)\n",
    "X_play.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = agent.predict(X_play)\n",
    "pred = ct.inverse_transform(pd.DataFrame(pred, columns=['start_at', 'end_at'], index=X_play.index))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
